import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, when
import psutil
# ğŸ”¹ Iniciar sesiÃ³n de Spark
spark = SparkSession.builder \
    .appName("Batch Processing - Sales Data") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .getOrCreate()

# ğŸ•’ Registrar tiempo de inicio
start_time = time.time()


try:
    print("ğŸ“Œ Iniciando procesamiento batch...")

    # ğŸ”¹ 1. Cargar datos desde HDFS (ExtracciÃ³n)
    file_path = "hdfs://localhost:9000/user/BigDataToto/datos/sales_data_IDL1.csv"
    sales_data = spark.read.csv(file_path, header=True, inferSchema=True)
    print("âœ… Datos cargados desde HDFS.")

    # ğŸ”¹ 2. Transformaciones
    ventas = sales_data.dropna().dropDuplicates()
    ventas = ventas.withColumn("Date", to_date(col("Date")))
    ventas = ventas.withColumn(
        "Categoria_Venta",
        when(col("Sales") > 1000, "Alta")
        .when(col("Sales").between(500, 1000), "Media")
        .otherwise("Baja")
    )
    print("ğŸ”„ Transformaciones completadas.")

    # ğŸ”¹ 3. Guardar los resultados en HDFS (Load)
    output_path = "hdfs://localhost:9000/user/BigDataToto/datos/ventas_procesadas"
    ventas.write.parquet(output_path, mode="overwrite")
    print("âœ… Datos procesados guardados en HDFS.")

    # ğŸ•’ Calcular tiempo de ejecuciÃ³n
    execution_time = time.time() - start_time
    print(f"â³ Tiempo de ejecuciÃ³n del batch: {execution_time:.2f} segundos")
    # Obtener uso de CPU y memoria
    cpu_usage = psutil.cpu_percent(interval=1)
    ram_usage = psutil.virtual_memory().percent
    print(f"ğŸ” Uso de CPU: {cpu_usage}%")
    print(f"ğŸ” Uso de RAM: {ram_usage}%")

except Exception as e:
    print(f"âŒ Error en el procesamiento batch: {str(e)}")

finally:
    spark.stop()
    print("ğŸ“Œ SesiÃ³n de Spark finalizada.")
